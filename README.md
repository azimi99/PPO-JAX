## PPO in JAX
Here are some Gymnasium environments with continuous observation spaces and discrete action spaces suitable for benchmarking this PPO JAX implementation:

- `CartPole-v1`
- `MountainCar-v0`
- `Acrobot-v1`
- `LunarLander-v3`
## Results
Check out our experiment results and interactive visualizations below:

<iframe src="https://wandb.ai/azimi/ppo%20jax/reports/JAX-PPO-Results--VmlldzoxMzYzNzMzMg" style="border:none;height:1024px;width:100%">

## Technical Writeup

A detailed technical writeup is coming soon.

Cheers!

## References

- **Proximal Policy Optimization Algorithms**  
    Schulman, J., Wolski, F., Dhariwal, P., Radford, A., & Klimov, O. (2017).  
    [arXiv:1707.06347](https://arxiv.org/abs/1707.06347)

- **CleanRL: High-quality single-file implementations of RL algorithms**  
    [CleanRL GitHub Repository](https://github.com/vwxyzjn/cleanrl)
